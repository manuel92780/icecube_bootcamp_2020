{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BootCamp_Diffuse_Fit_Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D5pFUM11WXle"
      },
      "source": [
        "![alt text](https://wiki.icecube.wisc.edu/images/a/a4/HESE75_skymap_with_events.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iem1cs_6Xvbo"
      },
      "source": [
        "# Diffuse Neutrinos #\n",
        "\n",
        "IceCube Bootcamp 2020 \\\\\n",
        "Madison, WI \n",
        "\n",
        "Manuel Silva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DaWTk59DY2kp"
      },
      "source": [
        "# Introduction and Goals #\n",
        "\n",
        "\n",
        "*  Learn what diffuse neutrinos are and how IceCube actually detects them\n",
        "*  Review of Poisson statistics and likelihood construction\n",
        "*  Run your own diffuse fit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "knWe0CYJZgu6"
      },
      "source": [
        "![alt text](https://www.researchgate.net/profile/J_Joutsenvaara/publication/304487324/figure/fig5/AS:492577876123650@1494451126031/Neutrino-energy-ranges-and-their-relative-intensities-according-to-their-source-of_W640.jpg)\n",
        "\n",
        "\n",
        "\n",
        "# Summary of diffuse neutrino fluxes for all energy ranges. \n",
        "\n",
        "- Cosmological neutrinos are neutrinos left over from the Big Bang\n",
        "- Solar neutrinos are byproducts of nuclear fusion reactions in the stellar core\n",
        "- Atmospheric neutrinos are created when cosmic rays bombard our atmosphere\n",
        "- Astrophysical neutrinos are created near cosmic accelarator sites along with cosmic rays\n",
        "- GZK neutrinos are created when UHE protons interact with the cosmic microwave background, the highest energy neutrinos in the universe!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uipWG37pb2nb"
      },
      "source": [
        "# Diffuse Neutrinos #\n",
        "\n",
        "*  Diffuse neutrinos are neutrinos that do not arrive at Earth from any particular source (isotropic), they are the total neutrinos that IceCube eventually detects\n",
        "*  As an experimentalist, you would analyze your data and test several models \n",
        "    *  The most generic model with smallest number of degrees of freedom is the single power law flux, this is denoted by an astrophysical spectral index $\\gamma$ and astrophysical normalization $\\Phi_0$ where the flux is defined as: $ \\Phi_0~ (\\frac{E}{100TeV})^{-\\gamma} $\n",
        "    * The choice for the 100 TeV is such that we can easily compute the flux of neutrinos at exactly 100 TeV\n",
        "    \n",
        "    * If your data sample contains atmospheric neutrinos, you will also model this flux and eventually include it in your fit. These models are complex and are not easily modeled via simple parametric functions, there it is usually included in your fit as some sort of scaling parameter\n",
        "    \n",
        "*  Theorists then use IceCube best fit to data so they can better predict new models, optimize existing models, reject models, etc....\n",
        "\n",
        "*  For reference, IceCube is capable of detecting both Atmospheric and Astrophysical Neutrinos, if you want high purity of one you would need an event selection to chose one over the other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "058UzG94lasF"
      },
      "source": [
        "# Observable #1 - Energy #\n",
        "\n",
        "*  The diffuse neutrino flux is highly dependent on energy, we must therefore have a good understanding of how we esimate the energy of detected neutrinos\n",
        "*  Recall that IceCube doesn't detect energy, it measures the light/charge deposited in each DOM\n",
        "  * Use a reconstruction algorithim to convert the light into energy, [summary of published algorithims](https://arxiv.org/pdf/1311.4767.pdf)\n",
        "*  This is our **first observable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VHeYze4XneeO"
      },
      "source": [
        "# Observable #2 - Zenith #\n",
        "\n",
        "*  Atmospheric neutrino flux is higly dependent on zenith, astrophysical flux is isotropic\n",
        "  *  In addition, your event selection may reject events depending on the direction\n",
        "* You can use the light deposited in IceCube, along with timing information, to predict the incoming direction of the neutrinos\n",
        "    *  Use a reconstruction algorithim to convert light and timing to direction, [summary of published algorithims](https://arxiv.org/pdf/astro-ph/0407044v1.pdf)\n",
        "*  This is our **second observable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xAIH4yBXo-3w"
      },
      "source": [
        "# Observable - Zenith #\n",
        "\n",
        "* Neutrinos need to traverse large amounts of Earth rock for large zeniths ($\\Theta > 80^{\\cdot}$)\n",
        "* For small zeniths ($\\Theta < 80^{\\cdot}$), neutrinos only need to travel through ice\n",
        "\n",
        "![alt text](https://icecube.wisc.edu/~msilva/BootCamp_2019/IceCube_zeniths.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wjNR6GUS8kUT"
      },
      "source": [
        "\n",
        "# Observable #3 - Flavor #\n",
        "*  Neutrinos can interact with protons/neutrons in the ice via electroweak interaction\n",
        "  * Neutral current (top) or charged current (bottom) deep-inelastic scattering dominates at these energies\n",
        "  *  Cannot extract information about the neutrino flavor from neutral currents since all neutrino flavors produce similar hadronic cascades\n",
        "  *  Eletrons produce electromagnetic cascades, muons produce tracks,  taus produce a \"double-bang\" (~18% taus produce a track)\n",
        "* It is very difficult to identify hadronic cascades from a neutral current interaction versus an electromagnetic cascades from electron-neutrinos chraged current interaction, so for today they are classified as the same\n",
        "\n",
        "  \n",
        "![alt text](https://icecube.wisc.edu/~msilva/BootCamp_2019/Diagrams.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_kSF64LfYCvl"
      },
      "source": [
        "# Observable #3 - Topology #\n",
        "  \n",
        "* Since we can't technically differentiate between electron neutrino and all other neutrino's hadronic cascade, we instead refer to this obserable as \"topology\"\n",
        "\n",
        "![alt text](https://icecube.wisc.edu/~msilva/BootCamp_2019/EventTopologies.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PI_96_PTYHRG"
      },
      "source": [
        "# Observables - Summary #\n",
        "*  For today's example, lets stick to using **energy, zenith, and topology as our observables **\n",
        "*  Hopefully, you will be able to repurpose your code in the future if you ever need to add more/different observables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4seWKJr_yzRt"
      },
      "source": [
        "# Counting Statistics #\n",
        "\n",
        "Consider a detector that counts particles. \n",
        "\n",
        "If our expectation of the rate (counts/s) is fixed over a given interval, and each count is independent of the previous or next one, the probability of seeing a count of **k** particles in your detector follows a poisson distribution.\n",
        "\n",
        "The number of particles we detect in a time **t** is a random variable.\n",
        "\n",
        "The probability, of detecting **k** counts when expecting **$\\lambda$** counts in time **t** is : \n",
        "\n",
        "\n",
        "$P(k | \\lambda)=\\frac{\\lambda^{k} e^{-\\lambda}}{k !}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QUTXEVFEyz05",
        "colab": {}
      },
      "source": [
        "#In code form\n",
        "def poisson(k, l):\n",
        "    return (l**k * np.exp(-l))/np.math.factorial(k)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJdxQxJs1NBK",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vvu4jWZF1Rg-"
      },
      "source": [
        "# What does this look like #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qHW0NKly1R38",
        "colab": {}
      },
      "source": [
        "|l = range(1, 10)[::2]\n",
        "\n",
        "for i in range(len(l)):\n",
        "    probs  = [poisson(k, l[i]) for k in range(0,20)]\n",
        "    plt.plot(range(0,20), probs, label='$\\\\lambda$ = %s'%(str(l[i])))\n",
        "\n",
        "plt.xlabel('Counts')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Poisson Distribution')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTZkQ78e1neq"
      },
      "source": [
        "# Why is this useful? #\n",
        "\n",
        "Collect IceCube data for a set amount of time, 1 year or maybe 10 years.\n",
        "What you have is a *detected* number of neutrinos (now referred to as an event), and you often try to estimate the *true* expectation.\n",
        "\n",
        "Let use math to descibe this now: the likelihood of $ \\lambda $ -expected events given k-detected events is: \n",
        "\n",
        "$\\Large \\mathcal{L}(\\lambda | k) =   P(k | \\lambda)=\\frac{\\lambda^{k} e^{-\\lambda}}{k !}$\n",
        "\n",
        "\n",
        "Given that you have already seen **k** events in your detector, then we can estimate the true expectation with whatever value **maximizes** the probability that you will detect that many events. (eg scan $\\lambda$ to maximize $\\Large \\mathcal{L}$).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2akvgW-A1rlb"
      },
      "source": [
        "# Let's fit some toy data # \n",
        "\n",
        "\n",
        "- To do this, you'll need to load this file: **toy_data_E-2.npy**\n",
        "\n",
        "    1) Download from: https://icecube.wisc.edu/~msilva/BootCamp_2019/toy_data_E-2.npy\n",
        "   \n",
        "    2a) Are you using virtual machine, put this file somwhere you access.\n",
        " \n",
        "    2b) I'm using google collab which gives you access to cpus and gpus via your google drive, just put the ipynb there and open with collab.\n",
        "\n",
        "- We are going to make some assumptions: \n",
        "\n",
        "    1) We have a spherical detector hovering in vacuum somehwere in deep space.\n",
        "    \n",
        "    2) Our detector is perfect (no systematics uncertainties)\n",
        "        \n",
        "    3) We have prior knowledge of the incoming flux which is, conveniently, a simple power law defined as:   $ \\large~~\\Phi_0~ E^{-\\gamma}$ \n",
        "    \n",
        "    *where gamma is the spectral index, and it's known in this case to be ($\\gamma$ = 2)\n",
        "    \n",
        "    4) We have been taking data continuously for ~100 years.\n",
        "    \n",
        "    \n",
        "- Our goal is to fit for the normalization $\\Phi_0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltTFEpVnEmM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  google . colab  import  drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w621dYXr17TO",
        "colab": {}
      },
      "source": [
        "data = np.load('gdrive/My Drive/toy_data_E-2.npy')\n",
        "\n",
        "bins = np.logspace(3, 6, 10)\n",
        "centers = (bins[1:] + bins[:-1])/2.\n",
        "h = np.histogram(data, bins=bins)\n",
        "plt.step(centers, h[0], where='mid', label='data')\n",
        "plt.semilogy(nonposy='clip')\n",
        "plt.semilogx()\n",
        "plt.xlabel('Energy (GeV)')\n",
        "plt.ylabel('Counts')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2bD4JSxi5gK5"
      },
      "source": [
        "- Now that we have our data, we can define the likelihood function, $\\mathcal{L}$ :\n",
        "\n",
        "\n",
        "$ \\Large \\mathcal{L}(\\vec{\\theta} | \\vec{d})=\\prod_{i} \\frac{\\left(\\lambda_{i}\\right)^{k_{i}} e^{-\\lambda_{i}}}{k_{i} !}$\n",
        "\n",
        "\n",
        "The only difference between the likelihood and the probability we defined earlier is that now you're taking into account that you have several bins. So you have an expectation $\\lambda_{i}$ and an observed value $k_{i}$ in every bin \"$i$\" \n",
        "\n",
        "It's often easier to work with the negative log-likelihood function, which is just  - log($\\mathcal{L}$) (any idea why?): \n",
        "\n",
        "$ \\Large \\log \\mathcal{L}(\\theta | \\vec{d})=\\sum_{i} k_{i} \\cdot \\log \\left(\\lambda_{i}\\right)-\\lambda_{i}-\\Gamma\\left[k_{i}+1\\right]$\n",
        "\n",
        "\n",
        "We want to maximize the probability that a certain expectation gives us the observed dataset. So we can maximize log($\\mathcal{L}$)  (or minimize negative log($\\mathcal{L}$))\n",
        "\n",
        "We will now define a function that returns our expectation in a certain bin, and another one that evaluates the likelihood.\n",
        "\n",
        "The **expectation** or number of events\n",
        "in a bin i is:\n",
        "\n",
        "$ \\Large \\lambda_i$ = $\\Phi_0~ \\int_{E_i} \\frac{dN}{dE} dE$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09FtPn7Y5grD",
        "colab": {}
      },
      "source": [
        "years = 365.*86400. #seconds\n",
        "livetime = 100.*years\n",
        "effA = 1e4 #cm^2 (cross-section of interaction * number of targets)\n",
        "\n",
        "def expectation(phi_0, i):\n",
        "    return phi_0*(1./(bins[i]) - 1./(bins[i+1]))*livetime*effA\n",
        "\n",
        "#We are given the spectral index, so our only free parameter in this case is the normalization $\\Phi_0$\n",
        "def log_likelihood(data, phi_0):\n",
        "    llh = []\n",
        "    for i in range(len(data)):\n",
        "        llh.append(-(data[i] * np.log(expectation(phi_0, i)) - expectation(phi_0, i) - special.loggamma(data[i]+1)))\n",
        "\n",
        "    llh = np.sum(np.asarray(llh)).real\n",
        "    return llh"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xr2GqtzA769J"
      },
      "source": [
        "# We have our likelihood! # \n",
        "\n",
        "Two most common ways to minimize our likelihood.\n",
        "\n",
        "1) The easy way is to scan your likelihood function and pick out the minimum. \n",
        "This can get computationally expensive and near impossible for large datasets with many free parameters\n",
        "\n",
        "2) The better way is to use a *minimizer*. Minimizers are special algorithms developed specifically to find the minima of a given function across an arbitrary number of dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Q5pg0398J5Q"
      },
      "source": [
        "# Lets start by scanning our likelihood #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XW6nygD8KnV",
        "colab": {}
      },
      "source": [
        "phis = np.logspace(-8, -4, 50)\n",
        "profile = [log_likelihood(h[0], phi) for phi in phis]\n",
        "plt.plot(phis, profile)\n",
        "plt.xlabel('$\\Phi_0$ [GeV$^{-1}$ cm$^{-2}$ s$^{-1}$ sr$^{-1}$]')\n",
        "plt.ylabel('- llh value', fontsize=16)\n",
        "plt.loglog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-T2DU8_J9ENY",
        "colab": {}
      },
      "source": [
        "norm_scanned = phis[np.where(profile == min(profile))[0][0]]\n",
        "print \"minimized llh found at \\Phi_0 = \" + str(norm_scanned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o9TAQ2wc9f02"
      },
      "source": [
        "# Let's use a minimizer #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bejp004S8xRB",
        "colab": {}
      },
      "source": [
        "import scipy.optimize as sp"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zK9Kp_FR8f5a",
        "colab": {}
      },
      "source": [
        "results = sp.minimize(lambda x: log_likelihood(h[0], *x),\n",
        "                           [3.e-7],\n",
        "                           method='L-BFGS-B',\n",
        "                           bounds=[[0,None]])\n",
        "\n",
        "print(results)\n",
        "norm_min = results.x[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MtprjQg862u",
        "colab": {}
      },
      "source": [
        "print('Percent difference between minmizer and scanned value:%2.1f %%'%((norm_min - norm_scanned)/(norm_min) * 100.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "548atT_x-SS1"
      },
      "source": [
        "The scanned value was ~10 percent different from the minimized value. \n",
        "\n",
        "The accuracy from the 1-dimensional scan depends on how many points you actually test. If we run more points, the minimum eventually converges. What happens if you test 1000 points? What about 1000 points with 2-dimensions? What about 10-dimensions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8XMV0UDf-dCG"
      },
      "source": [
        "# Now we can plot our fitted function against the data#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Svwoz4Mj-hYD",
        "colab": {}
      },
      "source": [
        "def power_law(norm, E):\n",
        "  return norm*E**(-2)\n",
        "\n",
        "\n",
        "bins = np.logspace(3, 6, 10)\n",
        "centers = (bins[1:] + bins[:-1])/2.\n",
        "lower = centers - bins[:-1] \n",
        "upper = bins[1:] - centers\n",
        "xerr = np.array([lower, upper])\n",
        "\n",
        "norm = norm_min\n",
        "dats = [expectation(norm_min, i) for i in range(len(centers))]\n",
        "\n",
        "plt.figure(figsize=[8,5])\n",
        "plt.step(bins, np.asarray(dats + [0]), where='post',linewidth=2, label='fit')\n",
        "plt.errorbar(centers, h[0], xerr=xerr, yerr=(np.sqrt(h[0])),capsize=4,fmt='o',color='k',alpha=0.9,label='data',linewidth=2)\n",
        "plt.semilogy(nonposy='clip')\n",
        "plt.semilogx()\n",
        "plt.legend(fontsize=15, loc='lower left', fancybox = True)\n",
        "plt.grid(ls='-.',which='both',alpha=0.3)\n",
        "plt.xlabel('Energy (GeV)',fontsize=15)\n",
        "plt.ylabel('Number of Events', fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRBvt3VObuo",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations! You have performed your first icecube binned likelihood analysis!#\n",
        "\n",
        "Many thanks to Ibrahim Safa for his help designing the example!"
      ]
    }
  ]
}